{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports And Global Variables\n",
    "This Cell Contains All Needed Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from access import Access, weights, Datasets\n",
    "import logging\n",
    "import geopandas as gpd\n",
    "from geopandas.tools import sjoin\n",
    "from shapely import wkt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate API Key\n",
    "\n",
    "Fill In With OSRM API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"\"  # Replace With Actual API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial File Setup\n",
    "\n",
    "This Cell Contains The Code Needed To get The Input Files Cleaned, Merged, And Filtered Correctly.\n",
    "\n",
    "- `2020_UA_COUNTY` Is from ESRIs Database.\n",
    "\n",
    "- `tl_2020_us_county` Is from Census Bureau Database.\n",
    "\n",
    "- `read_and_prepare_data` Reads And Cleans The Files To Ensure They Can Merge Correctly.\n",
    "\n",
    "- `filter_by_state` Uses The State Abbreviation To State Code Dictionary To Get The File Contents For The Desired State.\n",
    "\n",
    "- `getCounties` Gets The Files That Have Information On The Counties Demographics.\n",
    "\n",
    "- `getLocations` Gets The File That has information On The providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/state-fips-dictionary.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/william/Desktop/ForRepo/finalRAAM.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/william/Desktop/ForRepo/finalRAAM.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/william/Desktop/ForRepo/finalRAAM.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Global state FIPS dictionary\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/william/Desktop/ForRepo/finalRAAM.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m state_fips \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/state-fips-dictionary.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/william/Desktop/ForRepo/finalRAAM.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m state_fips \u001b[39m=\u001b[39m {state_fips[\u001b[39m'\u001b[39m\u001b[39mAbbreviation\u001b[39m\u001b[39m'\u001b[39m]: state_fips[\u001b[39m'\u001b[39m\u001b[39mFIPS\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/william/Desktop/ForRepo/finalRAAM.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_and_prepare_data\u001b[39m():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/state-fips-dictionary.csv'"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Global state FIPS dictionary\n",
    "state_fips = pd.read_csv('/state-fips-dictionary.csv')\n",
    "state_fips = {state_fips['Abbreviation']: state_fips['FIPS']}\n",
    "\n",
    "def read_and_prepare_data():\n",
    "    \"\"\"\n",
    "    Reads And Passes County Geographic Data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame Of Counties\n",
    "    gpd.GeoDataFrame: A GeoDataFrame Of Counties\n",
    "    \"\"\"\n",
    "    # Load And prepare The county population data\n",
    "    df = pd.read_csv('./2020_UA_COUNTY.csv', usecols=['STATE', 'COUNTY', 'POP_COU'])\n",
    "    df['STATE'] = df['STATE'].astype(str).str.zfill(2)  # Ensure state codes are two digits\n",
    "    df['COUNTY'] = df['COUNTY'].astype(str).str.zfill(3)  # Ensure county codes are three digits\n",
    "    df['GEOID'] = df['STATE'] + df['COUNTY']\n",
    "    # Removing ',' from The population To create an integer\n",
    "    df['Pop'] = df['POP_COU'].str.replace(',', '').astype(int) \n",
    "    \n",
    "    # Load And prepare The shapefile data\n",
    "    shp = gpd.read_file('./tl_2020_us_county/tl_2020_us_county.shp')\n",
    "    shp['GEOID'] = shp['GEOID'].astype(str)\n",
    "\n",
    "    return df, shp\n",
    "\n",
    "\n",
    "def filter_by_state(shp, df, state):\n",
    "    \"\"\"\n",
    "    Merges And Filters Geographic Files\n",
    "    \n",
    "    Args:\n",
    "    shp (gpd.GeoDataFrame): A Geopandas Dataframe Of County Data\n",
    "    df (pd.DataFrame): A Dataframe Of County Data\n",
    "    state (str): A State Abbreviation\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame Of Counties\n",
    "    \"\"\"\n",
    "    # Merge And filter The data based on state Abbreviation\n",
    "    merged_data = shp.merge(df, on='GEOID', how='inner')\n",
    "    if state in state_fips:\n",
    "        merged_data = merged_data[merged_data['STATE'] == state_fips[state]]\n",
    "    elif state == 'AR_LA_MS':\n",
    "        states = ['05', '22', '28']\n",
    "        merged_data = merged_data[merged_data['STATE'].isin(states)]\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "def getCounties(state):\n",
    "    \"\"\"\n",
    "    Reads And Saves Speicifed Geographic Data\n",
    "    \n",
    "    Args:\n",
    "    state (str): A State Abbreviation\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame Of Counties\n",
    "    \"\"\"\n",
    "    df, shp = read_and_prepare_data()\n",
    "    filtered_data = filter_by_state(shp, df, state)\n",
    "\n",
    "    # Creating a GeoDataFrame\n",
    "    geo_df = gpd.GeoDataFrame(\n",
    "        filtered_data, \n",
    "        geometry=gpd.points_from_xy(filtered_data['INTPTLON'].astype(float), filtered_data['INTPTLAT'].astype(float))\n",
    "    )\n",
    "    geo_df = geo_df[['GEOID', 'Pop', 'geometry']]\n",
    "    \n",
    "    return geo_df\n",
    "\n",
    "def getLocations(state, type):\n",
    "    \"\"\"\n",
    "    Reads And Saves Speicifed Provider Data For A Specified State\n",
    "    \n",
    "    Args:\n",
    "    state (str): A State Abbreviation\n",
    "    type (str): A Type Of Provider\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame Of Providers\n",
    "    \"\"\"\n",
    "    # Getting The Providers Data\n",
    "    df = gpd.read_file(f'./{type}/Input/{state}_{type}.geojson')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Routes Using OSRM\n",
    "\n",
    "This Cell Uses OSRM To Map Sources To destination.\n",
    "\n",
    "- `extract_coords` Function Gets X, Y Coordinates From Point Object.\n",
    "\n",
    "- `call_osrm_api` Function Takes Control Of The Flow To And From OSRM's API. \n",
    "\n",
    "- `get_distances` Populates The Distance Matrix.\n",
    "\n",
    "- `mapRoutes` Gets The Input Files And Starts The Process Of Mapping The Routes. Once The Distance Matrix Has Been Returned, It Melts It For Ease Of Use.\n",
    "\n",
    "- Hosting An Instance Of OSRM Is Also A Valueable Option. \n",
    "\n",
    "- *Instructions For That Can Be Found Here: https://github.com/Project-OSRM/osrm-backend*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coords(df):\n",
    "    \"\"\"\n",
    "    Extracts X,Y From Point Object.\n",
    "    \n",
    "    Args:\n",
    "    df (GeoDataFrame): A GeoDataFrame With A 'geometry' Column.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An Array Of X And Y Coordinates.\n",
    "    \"\"\"\n",
    "    # Getting X,Y from point object\n",
    "    df['X'] = df['geometry'].apply(lambda p: p.x)\n",
    "    df['Y'] = df['geometry'].apply(lambda p: p.y)\n",
    "    return df[['X', 'Y']].to_numpy()\n",
    "\n",
    "def call_osrm_api(src_coords, dest_coords, sources, destinations, delay, max_retries=3):\n",
    "    \"\"\"\n",
    "    Call The OSRM API To Get Distances Between Source And Destination Coordinates.\n",
    "\n",
    "    Args:\n",
    "    src_coords (numpy.ndarray): Array Of Source Coordinates.\n",
    "    dest_coords (numpy.ndarray): Array Of Destination Coordinates.\n",
    "    sources (str): String Of Source Indices For OSRM API.\n",
    "    destinations (str): String Of Destination Indices For OSRM API.\n",
    "    delay (int): Delay Between API Calls in Seconds.\n",
    "    max_retries (int): Maximum Number Of Retries For The API Call.\n",
    "\n",
    "    Returns:\n",
    "    list: A List Of Distance Values.\n",
    "    \"\"\"\n",
    "    coords = np.vstack((src_coords, dest_coords))\n",
    "    coordinates_str = \";\".join([f\"{x},{y}\" for x, y in coords])\n",
    "    osrm_endpoint = f\"http://router.project-osrm.org/table/v1/driving/{coordinates_str}?sources={sources}&destinations={destinations}&annotations=distance\"\n",
    "    headers = requests.utils.default_headers()\n",
    "    headers.update({'User-Agent': 'My User Agent 1.0'})\n",
    "\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            response = requests.get(osrm_endpoint, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return json.loads(response.content)['distances']\n",
    "        except requests.exceptions.RequestException as err:\n",
    "            print(f\"Attempt {attempts + 1} failed: {err}\")\n",
    "            attempts += 1\n",
    "            time.sleep(delay)\n",
    "\n",
    "    raise Exception(f\"Failed to get distances after {max_retries} attempts for sources: {sources} and destinations: {destinations}.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_distances(src_coords, dest_coords, src_batch_size=10, dest_batch_size=50, delay=1):\n",
    "    \"\"\"\n",
    "    Get Distances Between Source And Destination Coordinates Using OSRM API.\n",
    "\n",
    "    Args:\n",
    "    src_coords (numpy.ndarray): Array Of Source Coordinates.\n",
    "    dest_coords (numpy.ndarray): Array Of Destination Coordinates.\n",
    "    src_batch_size (int): Batch Size For Source Coordinates.\n",
    "    dest_batch_size (int): Batch Size For Destination Coordinates.\n",
    "    delay (int): Time Delay Between API Calls In Seconds.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A Matrix Of Distance Values.\n",
    "    \"\"\"\n",
    "    # Initialize Matrix\n",
    "    distanceMat = np.zeros((len(src_coords), len(dest_coords)))\n",
    "    # Iterate Through Coordinates\n",
    "    for i in tqdm(range(0, len(src_coords), src_batch_size)):\n",
    "        batch_src_coords = src_coords[i:i + src_batch_size]\n",
    "        sources = \";\".join(map(str, range(len(batch_src_coords))))\n",
    "        for j in range(0, len(dest_coords), dest_batch_size):\n",
    "            batch_dest_coords = dest_coords[j:j + dest_batch_size]\n",
    "            destinations = \";\".join(map(str, range(len(batch_src_coords), len(batch_src_coords) + len(batch_dest_coords))))\n",
    "            distances = call_osrm_api(batch_src_coords, batch_dest_coords, sources, destinations, delay)\n",
    "            if distances is not None:\n",
    "                distanceMat[i:i + src_batch_size, j:j + dest_batch_size] = distances\n",
    "            time.sleep(delay)\n",
    "    return distanceMat\n",
    "\n",
    "\n",
    "def mapRoutes(state, ID, type):\n",
    "    \"\"\"\n",
    "    Maps Routes Between Counties And Locations Of A Specified Provider Within A State.\n",
    "\n",
    "    Args:\n",
    "    state (str): State Abbreviation.\n",
    "    ID (str): ID Column Of Provider.\n",
    "    type (str): Type Of Locations To Be Mapped.\n",
    "\n",
    "    Returns:\n",
    "    None: Saves The Result To A CSV File.\n",
    "    \"\"\"\n",
    "\n",
    "    df1 = getCounties(state)\n",
    "    df2 = getLocations(state, type)\n",
    "\n",
    "    coords1 = extract_coords(df1)\n",
    "    coords2 = extract_coords(df2)\n",
    "    \n",
    "    print(f'{state} has {len(coords1)} counties And {len(coords2)} {ID}s.')\n",
    "\n",
    "    # Parameters To Speed Up OSRM (change these as needed)\n",
    "    src_batch_size = 100\n",
    "    dest_batch_size = 100\n",
    "    delay = 2\n",
    "\n",
    "    distanceMat = get_distances(coords1, coords2, src_batch_size, dest_batch_size, delay)\n",
    "\n",
    "    distance_df = pd.DataFrame(distanceMat, index=df1['GEOID'], columns=df2[f'{ID}'])\n",
    "    # Melting The dataframe For Ease Of Use\n",
    "    melted_df = pd.melt(distance_df.reset_index(), id_vars='GEOID', value_vars=distance_df.columns, var_name=f'destination ({type})', value_name='cost (distance)')\n",
    "    melted_df = melted_df.rename(columns={'GEOID': 'source (GEOID)'})\n",
    "    melted_df.to_csv(f'./{type}/Output/{state}_melted_{type}_Distance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting The GEOID From The Provider's Coordinates\n",
    "\n",
    "This Cell Focuses On Using The Census Bureau API To Get The GEOID Or CountyFIPS Of Each County That there Is A Provider. This Is Done Using The Coordaintes Of Each Provider. \n",
    "\n",
    "- `get_geoid_from_coords` Takes In A Set Of Coordinates, Connects To The API, And Returns The GEOID At The County Level For Those Coodinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geoid_from_coords(latitude, longitude):\n",
    "    \"\"\"\n",
    "    Retrieves The GEOID For Given Latitude And Longitude Using The Census Bureau's Geocoding API.\n",
    "    \n",
    "    Args:\n",
    "    latitude (float): Latitude Of The Provider Location.\n",
    "    longitude (float): Longitude Of The Provider Location.\n",
    "\n",
    "    Returns:\n",
    "    str: GEOID Of The location.\n",
    "    \"\"\"\n",
    "    base_url = \"https://geocoding.geo.census.gov/geocoder/geographies/coordinates\"\n",
    "    params = {\n",
    "        \"x\": longitude,\n",
    "        \"y\": latitude,\n",
    "        \"benchmark\": \"Public_AR_Census2020\",\n",
    "        \"vintage\": \"Census2010_Census2020\",\n",
    "        \"format\": \"json\",\n",
    "        \"key\": API_KEY\n",
    "    }\n",
    "    # Making Request To API\n",
    "    response = requests.get(base_url, params=params)\n",
    "    data = response.json()\n",
    "    try:\n",
    "        # Parsing And returning results\n",
    "        geoid = data['result']['geographies']['Counties'][0]['GEOID']\n",
    "        return geoid\n",
    "    except (KeyError, IndexError):\n",
    "        print(\"Could not find GEOID for The provided coordinates.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supply And Demand Matrices\n",
    "\n",
    "This Cell Generates 3 Tables:\n",
    "\n",
    "- `getProviders` Creates A Supply Table Based On The Number Of Providers Per County (when observing Hospitals It Is Number Of Hospital Beds Rather Than Number Of providers)\n",
    "\n",
    "- `getTimes` Uses Both OSRM And Cenus Bureau API Outputs To Create A File That Will Show `[Source, Destination, Time]` *Source And Destination Are GEOIDs*\n",
    "\n",
    "- `getPop` Simply Gets The Population Of Each County. \n",
    "\n",
    "These Tables Are Used For The Input Of RAAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getProviders(provider, col, type, state):\n",
    "    \"\"\"\n",
    "    Aggregates Provider Data By GEOID And Merges With County Data.\n",
    "    \n",
    "    Args:\n",
    "    provider (DataFrame): Provider Data.\n",
    "    col (str): ID Column In Provider Data.\n",
    "    type (str): Type Of Provider.\n",
    "    state (str): State Abbreviation.\n",
    "\n",
    "    Returns:\n",
    "    None: Saves The Result To A CSV File.\n",
    "    \"\"\"\n",
    "    t1 = getCounties(state)\n",
    "    t2 = provider\n",
    "\n",
    "    t1['GEOID'] = t1['GEOID'].astype(int)\n",
    "    t2['GEOID'] = t2['GEOID'].astype(int)\n",
    "\n",
    "    # To Make Sure Hospitals Are Counted By Beds And Not Providers\n",
    "    if type == 'Hospitals':\n",
    "        doctor_counts = t2.groupby('GEOID')['Licensed_A'].sum().reset_index()\n",
    "        doctor_counts = doctor_counts.rename(columns={'Licensed_A': f'{type}'})\n",
    "    else:\n",
    "        doctor_counts = t2.groupby('GEOID')[f'{col}'].count().reset_index()\n",
    "        doctor_counts = doctor_counts.rename(columns={f'{col}': f'{type}'})\n",
    "\n",
    "    # Joining To get population And coordinates\n",
    "    t3 = pd.merge(t1, doctor_counts, on='GEOID', how='left')\n",
    "    t3[f'{type}'] = t3[f'{type}'].fillna(0).astype(int)\n",
    "    t3 = t3[['GEOID', 'Pop', f'{type}', 'geometry']]\n",
    "    t3.to_csv(f'./{type}/SupplyDemand/{state}/{state}_{type}.csv', index=False)\n",
    "\n",
    "    # Printing Number Of Distinct Counties With A Provider\n",
    "    print(f\"Total Number Of Proivers ({type}):\", len(t2))\n",
    "    print(f\"Counties With Atleast 1 Provider ({type}):\", doctor_counts['GEOID'].nunique())\n",
    "    print(\"Total Counties\", t1['GEOID'].nunique())\n",
    "    unmatched_geo_ids = doctor_counts[~doctor_counts['GEOID'].isin(t1['GEOID'])]\n",
    "    if len(unmatched_geo_ids) >= 1:\n",
    "        print(f'Unmatched GEOIDs: {unmatched_geo_ids}')\n",
    "\n",
    "def getTimes(provider, col, type, state):\n",
    "    \"\"\"\n",
    "    Merges Calculated Time Data With Provider Data.\n",
    "\n",
    "    Args:\n",
    "    provider (DataFrame): Provider Data.\n",
    "    col (str): ID Column In Provider Data.\n",
    "    type (str): Type Of Provider.\n",
    "    state (str): State Abbreviation.\n",
    "\n",
    "    Returns:\n",
    "    None: Saves The Result To A CSV File.\n",
    "    \"\"\"\n",
    "    provider['GEOID'] = provider['GEOID'].astype(int)\n",
    "    times = pd.read_csv(f'./{type}/Output/{state}_melted_{type}_Distance.csv', low_memory=False)\n",
    "    times['source (GEOID)'] = times['source (GEOID)'].astype(int)\n",
    "\n",
    "    # For These Providers, Coordinates Are Used As Keys\n",
    "    if type in ['MentalHealth', 'PWMH']:\n",
    "        # To Ensure The Merge Is Consistent The Coordinates Go Back To A Point Object\n",
    "        times['destination_geom'] = times['destination (geometry)'].apply(wkt.loads)\n",
    "        times = gpd.GeoDataFrame(times, geometry='destination_geom')\n",
    "        t3 = sjoin(times, provider[['GEOID', 'geometry']], op='intersects', how='left')\n",
    "        t3 = t3[['source (GEOID)', 'GEOID', 'cost (distance)']]\n",
    "    else:\n",
    "        t3 = pd.merge(times, provider, left_on=f'destination ({type})', right_on=col, how='left')\n",
    "\n",
    "    t3 = t3[['source (GEOID)', 'GEOID', 'cost (distance)']]\n",
    "    t3.rename(columns={'GEOID': 'destination (GEOID)'}, inplace=True)\n",
    "    t3 = t3.drop_duplicates(subset=['source (GEOID)', 'destination (GEOID)'])\n",
    "    t3.to_csv(f'./{type}/SupplyDemand/{state}/{state}_Times_{type}.csv', index=False)\n",
    "\n",
    "def getPop(state, type):\n",
    "    \"\"\"\n",
    "    Retrieves Population Data For A State And Type Of Service.\n",
    "    \n",
    "    Args:\n",
    "    state (str): State Abbreviation.\n",
    "    type (str): Type Of Provider.\n",
    "\n",
    "    Returns:\n",
    "    None: Saves The Result To A CSV File.\n",
    "    \"\"\"\n",
    "    t1 = getCounties(state)\n",
    "    t1['GEOID'] = t1['GEOID'].astype(int)\n",
    "    t3 = t1[['GEOID', 'Pop', 'geometry']]\n",
    "    t3.to_csv(f'./{type}/SupplyDemand/{state}/{state}_Pop_{type}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running RAAM\n",
    "\n",
    "This Cell Does The Analysis Of RAAM Using The Supply, Demand, And Times Tables Created. Multiple Sets Of Weights Are Used. \n",
    "\n",
    "- `RAAM` Runs RAAM Analysis And Saves Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RAAM(state, type):\n",
    "    \"\"\"\n",
    "    Performs RAAM Analysis For A Specified State And Type Of Provider.\n",
    "\n",
    "    Args:\n",
    "    state (str): State Abbreviation.\n",
    "    type (str): Type Of Provider.\n",
    "\n",
    "    Returns:\n",
    "    None: Saves The Result To A CSV File.\n",
    "    \"\"\"\n",
    "    # Supressing Warning RAAM generates\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    # Initializing Supply And Demand Tables\n",
    "    pop = pd.read_csv(f'./{type}/SupplyDemand/{state}/{state}_Pop_{type}.csv')\n",
    "    pop = gpd.GeoDataFrame(pop, geometry=gpd.GeoSeries.from_wkt(pop[\"geometry\"]), crs=\"epsg:3005\")\n",
    "\n",
    "    provider = pd.read_csv(f'./{type}/SupplyDemand/{state}/{state}_{type}.csv')\n",
    "    provider = gpd.GeoDataFrame(provider, geometry=gpd.GeoSeries.from_wkt(provider[\"geometry\"]), crs=\"epsg:3005\")\n",
    "\n",
    "    times = pd.read_csv(f'./{type}/SupplyDemand/{state}/{state}_Times_{type}.csv')\n",
    "    times['cost (distance)'] *= 0.000621371  # Convert meters To miles\n",
    "    times = times.drop_duplicates(subset=['source (GEOID)', 'destination (GEOID)'])\n",
    "\n",
    "    # Creating Access Object\n",
    "    distanceA = Access(\n",
    "        demand_df=pop, demand_index=\"GEOID\", demand_value=\"Pop\",\n",
    "        supply_df=provider, supply_index=\"GEOID\", supply_value=type,\n",
    "        cost_df=times, cost_origin=\"source (GEOID)\", cost_dest=\"destination (GEOID)\", cost_name=\"cost (distance)\"\n",
    "    )\n",
    "\n",
    "    # Changing CRS Code\n",
    "    distanceA.demand_df = distanceA.demand_df.to_crs(epsg=3528)\n",
    "    distanceA.supply_df = distanceA.supply_df.to_crs(epsg=3528)\n",
    "\n",
    "    # Running RAAM With Different Sets Of Weights\n",
    "    def RAAMDis(A):\n",
    "        A.raam(name=\"raamDis\", tau=60)\n",
    "        A.raam(name=\"raam30Dis\", tau=30)\n",
    "        A.access_df.sort_values(by=f\"raamDis_{type}\").dropna().head()\n",
    "        A.score(name=\"raamDis_combo\", col_dict={f'raamDis_{type}': 0.8})\n",
    "        return A\n",
    "\n",
    "    # Saving Results\n",
    "    distanceA = RAAMDis(distanceA)\n",
    "    distanceA.log.setLevel(logging.WARNING)\n",
    "    distanceA.norm_access_df['County'] = distanceA.norm_access_df.index.astype(str).str.slice(0, 5)\n",
    "    df = distanceA.norm_access_df\n",
    "    df.set_index(distanceA.norm_access_df.index)\n",
    "    df.rename(columns={'raamDis_combo': f'raamDis_combo_{type}'}, inplace=True)\n",
    "    df.to_csv(f\"./{type}/RAAM/{state}_{type}_distance_results.csv\")\n",
    "    print(f'RAAM Results Saved At: ./{type}/RAAM/{state}_{type}_distance_results.csv \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controller \n",
    "\n",
    "This Cell Calls All Functions. \n",
    "- `process_state_data` Calls The Functions In The Order Needed To Run The Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping Routes With OSRM:\n",
      "AR has 75 counties And 11 NPIs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting GEOID From Coodinates:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:06<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Supply/Demand/Times Tables:\n",
      "Total Number Of Proivers (OBGYNsMedicaid): 11\n",
      "Counties With Atleast 1 Provider (OBGYNsMedicaid): 6\n",
      "Total Counties 75\n",
      "\n",
      "Running RAAM:\n",
      "RAAM Results Saved At: ./OBGYNsMedicaid/RAAM/AR_OBGYNsMedicaid_distance_results.csv \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_state_data(state, provider_type, id_field):\n",
    "    \"\"\"\n",
    "    Main Function To process State Data For A Specific State And Provider Type.\n",
    "\n",
    "    Args:\n",
    "    state (str): State Abbreviation.\n",
    "    provider_type (str): Type Of Provider.\n",
    "    id_field (str): ID Field Name In Provider Data.\n",
    "\n",
    "    Returns:\n",
    "    None: Executes The Data Processing Workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Creating Directories\n",
    "    os.makedirs(f'./{provider_type}/Output/', exist_ok=True)\n",
    "    os.makedirs(f'./{provider_type}/SupplyDemand/', exist_ok=True)\n",
    "    os.makedirs(f'./{provider_type}/SupplyDemand/{state}', exist_ok=True)\n",
    "\n",
    "    os.makedirs(f'./{provider_type}/RAAM/', exist_ok=True)\n",
    "    print('Mapping Routes With OSRM:')\n",
    "    mapRoutes(state, id_field, provider_type)\n",
    "    provider = getLocations(state, provider_type)\n",
    "    # Checking To see If The Key Will Be Coordiantes\n",
    "    if provider_type == 'MentalHealth' or provider_type == 'PWMH':\n",
    "        # If It Is Then Use Point Objects\n",
    "        provider['geometry_str'] = provider['geometry'].apply(\n",
    "            lambda x: wkt.dumps(x))\n",
    "\n",
    "    # Calling Method For Census Bureau API\n",
    "    print('\\nGetting GEOID From Coodinates:')\n",
    "\n",
    "    def geoid_for_row(row):\n",
    "        point = row['geometry']\n",
    "        longitude = point.x\n",
    "        latitude = point.y\n",
    "        geoid = get_geoid_from_coords(latitude, longitude)\n",
    "        return(str(int(geoid)))\n",
    "    \n",
    "    tqdm.pandas() \n",
    "    provider['GEOID'] = provider.progress_apply(geoid_for_row, axis=1)\n",
    "\n",
    "    # Generating Supply And Demand Tables\n",
    "    print('\\nCreating Supply/Demand/Times Tables:')\n",
    "\n",
    "    getProviders(provider, id_field, provider_type, state)\n",
    "    getTimes(provider, id_field, provider_type, state)\n",
    "    getPop(state, provider_type)\n",
    "\n",
    "    print('\\nRunning RAAM:')\n",
    "    RAAM(state, provider_type)\n",
    "\n",
    "### Provider Keys: ###\n",
    "# Medicare: NPI\n",
    "# Hospitals: HIFLD_ID\n",
    "# HealthCenters: BPHC_Assig\n",
    "# MentalHealth: geometry\n",
    "# PWMH: geometry\n",
    "\n",
    "\n",
    "process_state_data('AR', 'OBGYNsMedicaid', 'NPI')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
